{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Solutions - NLP Tutorial Jax - Haiku Language Modeling - Classification",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7kNszOZ-9Vto"
   },
   "source": [
    "# Natural Language Processing Tutorial\n",
    "\n",
    "Welcome to the second lab session of the [Mediterranean Machine Learning  summer school](https://www.m2lschool.org/home)!\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/m2lschool/tutorials2021/blob/main/2_nlp/NLP_tutorial_solutions.ipynb)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vG3iXLXskshZ"
   },
   "source": [
    "In this tutorial, you will learn the fundamental components and main tasks in the Natural Language Processing (MLP) domain.\n",
    "\n",
    "The tutorial is structured as follows:\n",
    "\n",
    "* Imports and downloads (shared across sections)\n",
    "* Part I: *Inner mechanisms of a vanilla Recurrent Neural Network (RNN)*\n",
    "* Part II: *Character-level language modeling with Long-Short Term Memory (LSTM) networks*\n",
    "* Part III: *Non-negative Matrix Factorization for topic modeling*\n",
    "* Part IV: *From traditional (bag-of-words) to pretrained representations (BERT) for text classification*\n",
    "\n",
    "\n",
    "All sections can be executed independently after running the initial import and download cells.\n",
    "\n",
    "The sections marked as \\[EXERCISE\\] contain cells with missing code that you should complete.\n",
    "\n",
    "\n",
    "Credits:\n",
    "**[Federico Bianchi](https://federicobianchi.io)**, **[Debora Nozza](https://dnozza.github.io/)** and **[Francesco Visin](https://scholar.google.it/citations?user=kaAnZw0AAAAJ)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bOsnT-ljkONc"
   },
   "source": [
    "### All Imports and Downloads\n",
    "\n",
    "Here we are going to install and import everything we are going to need for this tutorial. \n",
    "\n",
    "**Note**: *You can double-click the title of the collapsed cells (as the ones below) to expand them and read their content.*"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ncB_BwEIkaVL",
    "cellView": "form"
   },
   "source": [
    "# @title Downloads and libraries installation\n",
    "%%capture\n",
    "!pip install git+https://github.com/deepmind/dm-haiku\n",
    "!pip install optax\n",
    "!pip install tensorflow-datasets\n",
    "!pip install emoji\n",
    "!pip install sentence-transformers"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "8D3UzYYZmMoi",
    "cellView": "form"
   },
   "source": [
    "# @title Imports\n",
    "import emoji\n",
    "from functools import partial\n",
    "import re\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import haiku as hk\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax.nn.initializers import glorot_normal, normal, glorot_uniform, zeros\n",
    "from jax import grad, jit, vmap, value_and_grad\n",
    "from jax import random\n",
    "from typing import Iterator, Mapping\n",
    "from jax import lax\n",
    "from jax.scipy.special import logsumexp\n",
    "from jax import ops\n",
    "import optax\n",
    "import tensorflow.compat.v2 as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qB-mm0Y892q7"
   },
   "source": [
    "# Part I. Vanilla RNN\n",
    "\n",
    "**Goal**: Train a vanilla RNN to predict the next value of a temporal series given its previous values.\n",
    "\n",
    "Specifically, we aim to predict\n",
    "\n",
    "$$ \n",
    "sin (x +t \\epsilon) \n",
    "$$ \n",
    "\n",
    "from\n",
    "\n",
    "$$ \n",
    "sin (x), sin (x + \\epsilon), ..., sin (x + (t-1) \\epsilon) \n",
    "$$\n",
    "\n",
    "In particular, we want the network to generate multiple predictions conditioned only on a few initial values. To do so, we will predict the next value of the function in a loop, conditioning on the value at the previous time-step (be it the initial, given, values or the ones predicted by the network at each previous step).\n",
    "\n",
    "To learn the prediction model, we will use **teacher forcing**. This means that *during training* we will condition the model on values *coming from the ground truth* (i.e., the real sequence), rather than the output produced by the model at $t-1$. This makes training easier, because errors do not compound (a bad prediction at time $t$ does not influence the next prediction at time $t+1$).\n",
    "\n",
    "At *inference time* (i.e., when we want to generate data from the model) we do not have access to the true sequence, so we must condition on the values *predicted by the model in previous time-steps*. This often leads to some compounding error, which generally makes it harder and harder to generate long sequences accurately.\n",
    "\n",
    "To alleviate this problem we will also experiment with **warm starting**, which amounts to feeding the model with values coming from the ground truth (as done when teacher forcing) *only for a few initial steps*, and then feed it with its previous predictions instead. The rationale for this is to make training easier initially, to help the model learn the true function, and then feed it with its predictions to become robust to imperfect data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vmERQ1xn9bjY"
   },
   "source": [
    "## Training data\n",
    "\n",
    "Let's store the sine wave over the interval $[0, 2\\pi]$ as our training data. This will be our **ground truth**.\n",
    "\n",
    "Note that differently from what is usually done, we don't have two separate training and test datasets here. Instead, we will train on random subsamples of the curve and then verify the ability of the network to generate the full curve when conditioned on a few initial values:\n",
    "\n",
    "* Predict a few future values from a random starting point $x_s$;\n",
    "* Generate the full trajectory, conditioned on a few initial points."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "NXIWoTiK9d1W"
   },
   "source": [
    "t = np.arange(0, 2*np.pi, 0.1).reshape(-1, 1)\n",
    "sin_t = np.sin(t)\n",
    "\n",
    "plt.scatter(t, sin_t)\n",
    "plt.xlabel('$t$')\n",
    "plt.ylabel('$sin(t)$')\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "qc7EG0zKPMlI",
    "cellView": "form"
   },
   "source": [
    "# @title Hyperparameters\n",
    "WARM_START = 10                   #@param {type:'integer'}\n",
    "TEACHER_FORCING = False           #@param {type:'boolean'}\n",
    "STEP_SIZE = 0.0001                #@param {type:'number'}\n",
    "UNROLL_LENGTH = 30                #@param {type:'integer'}\n",
    "HIDDEN_DIMENSION = 20             #@param {type:'integer'}\n",
    "NUM_ITERATIONS = 10000            #@param {type:'integer'}\n",
    "REPORTING_INTERVAL = 2000         #@param {type:'integer'}\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HaZog6XYk20n"
   },
   "source": [
    "As you have seen in the introductory lab, to use pseudo-random functions in JAX we need to instantiate a random number generator, and pass it explicitely to all the operations that work with random numbers (e.g. model initialization, dropout, etc, ...)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "VrvlPPKIilGL"
   },
   "source": [
    "rnd_key = random.PRNGKey(1)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I56S_jw-POCI"
   },
   "source": [
    "## RNN cell \\[EXERCISE\\]\n",
    "\n",
    "Implement a basic RNN cell using *jax.numpy (jnp)* functions\n",
    "\n",
    "$$ h_t = f( Wx_t + Vh_{t-1}  + b) $$\n",
    "        \n",
    "Where,\n",
    "\n",
    "* $x_t$ input at time $t$,\n",
    "* $h_t$ hidden state at time $t$,\n",
    "* $W$ input-to-hidden mapping (trainable),\n",
    "* $V$ hidden-to-hidden mapping (trainable),\n",
    "* $b$ bias (trainable),\n",
    "* $f$ non-linearity chosen (we use *tanh*)\n",
    "\n",
    "Start by implementing a function to return the initial parameters:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "lflBFfKGkt10"
   },
   "source": [
    "def initialize_parameters(rnd_key):\n",
    "    \"\"\"\n",
    "    Initialize and return the Vanilla RNN parameters.\n",
    "\n",
    "    Args:\n",
    "        rnd_key: random key\n",
    "\n",
    "    Returns:\n",
    "        A dict of weights with keys `V`, `W`, `bias`, `decoder`, `decoder_bias`.\n",
    "    \"\"\"\n",
    "\n",
    "    input_dimension = 1   # the input is a scalar\n",
    "    output_dimension = 1  # the output is a scalar\n",
    "\n",
    "    ######################\n",
    "    #   YOUR CODE HERE   #\n",
    "    ######################\n",
    "    # Initialize the Vanilla RNN parameters\n",
    "    # using a glorot uniform distribution for W, \n",
    "    # V, decoder and zero values for bias and \n",
    "    # decoder_bias. Pay attention to the correct \n",
    "    # shapes of all parameters.\n",
    "\n",
    "    #### Solution:\n",
    "    keys = random.split(rnd_key, 5)\n",
    "    params = {\n",
    "        'V' : glorot_uniform()(keys[0], (HIDDEN_DIMENSION, HIDDEN_DIMENSION)),\n",
    "        'W' : glorot_uniform()(keys[1], (input_dimension, HIDDEN_DIMENSION)),\n",
    "        'bias' : zeros(keys[2], (HIDDEN_DIMENSION, )),\n",
    "        'decoder' : glorot_uniform()(keys[3], (\n",
    "            HIDDEN_DIMENSION, output_dimension)),\n",
    "        'decoder_bias' : zeros(keys[4], (output_dimension,)),\n",
    "    }\n",
    "\n",
    "    assert params is not None, 'Params should be initialized'\n",
    "\n",
    "    return params"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CR5Cgb-IfgRS"
   },
   "source": [
    "and then implement the RNN core itself:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "JrScN3TsP8Bv"
   },
   "source": [
    "def RNN_cell(params, x_t, current_state):\n",
    "    \"\"\"\n",
    "    This function will be called in a loop, when RNN core is connected to\n",
    "    inputs and previous states.\n",
    "\n",
    "    Args:\n",
    "        params: neural network paramters\n",
    "        x_t: jax DeviceArray containing the current input `x_{t}`\n",
    "        prev_state: jax DeviceArray containing the previous state `h_{t-1}`\n",
    "\n",
    "    Returns:\n",
    "        A pair of RNN embedding and state.\n",
    "    \"\"\"\n",
    "\n",
    "    ######################\n",
    "    #   YOUR CODE HERE   #\n",
    "    ######################\n",
    "    # Compute the output of the RNN Cell by implementing the equation above.\n",
    "    output = None\n",
    "\n",
    "    ### Solution\n",
    "    input_to_hidden  = jnp.dot(x_t, params['W'])\n",
    "    hidden_to_hidden =  jnp.dot(current_state, params['V'])\n",
    "    activation = input_to_hidden + hidden_to_hidden + params['bias']\n",
    "    output = jnp.tanh(activation)\n",
    "\n",
    "    assert output is not None, 'The RNN cell must compute the output'\n",
    "\n",
    "    # The RNN cell returns pairs of (o_t, h_t), respectively the output and the \n",
    "    # state at time $t$. For vanilla RNN these are the same.\n",
    "    return output, output\n",
    "\n",
    "def get_initial_rnn_state(rnd_key):\n",
    "    return zeros(rnd_key, (HIDDEN_DIMENSION,))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yb3iTta8Geea"
   },
   "source": [
    "Now let's check that the output of the RNN is as expected."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "w-3wH95dzvxE"
   },
   "source": [
    "rnd_key, k1, k2 = random.split(rnd_key, 3)\n",
    "params = initialize_parameters(k1)\n",
    "single_input = sin_t[0]\n",
    "initial_state = get_initial_rnn_state(k2)\n",
    "\n",
    "# RNN_cell returns two vectors, the output and the state. When the input is a \n",
    "# single element (i.e., not a sequence) each should have shape HIDDEN_DIMENSION.\n",
    "assert len(RNN_cell(params, single_input, initial_state)[0]) == HIDDEN_DIMENSION\n",
    "assert len(RNN_cell(params, single_input, initial_state)[1]) == HIDDEN_DIMENSION"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QV1-TMimQgPD"
   },
   "source": [
    "## Next element prediction [EXERCISE]\n",
    "\n",
    "The `predict` method defined below supports both *teacher forcing* and *warmup*. For educational purposes, we unrolled the RNN loop manually so it is easier to understand and debug the code. You need to implement the missing bits, applying the `RNN_cell` and returning a sequence of scalar predictions.\n",
    "\n",
    "The `sin(x)` input sequence is composed of scalars, which is what the `RNN_cell` expects so you don't need to preprocess them. The predictions of the `RNN_cell` have shape `(HIDDEN_DIMENSION,),` so you need to apply the $\\textbf{decoder}$ to recover the sequence of scalars we want to predict.\n",
    "\n",
    "When you are done debugging, uncomment the **`@jit` decorator** to ensure the code is optimized and runs faster."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "4aNfb-h6t-31"
   },
   "source": [
    "@jax.partial(jax.jit, static_argnums=(3,))\n",
    "def predict(params, initial_rnn_state, inputs, training=True):\n",
    "    \"\"\"Predict the next value of a sequence.\n",
    "\n",
    "    Here we compute *one-step predictions*, i.e., we predict `sin_{t+1}` from \n",
    "    `sin_{t}`. Note however that depending on the values of the TEACHER_FORCING \n",
    "    and WARM_START parameters, sometimes we feed the model with the actual\n",
    "    `sin_{t}` coming from the ground truth (i.e., `inputs`) and some other times\n",
    "    with the last prediction of the model (which might or might not be \n",
    "    accurate). This means that in some cases not all the values of `inputs` will\n",
    "    be used for training.\n",
    "\n",
    "    This code a simplified implementation of `hk.static_unroll` for educational \n",
    "    purposes, do not use it in actual code. Statically unrolling the network has\n",
    "    some caveats, e.g., it won't behave as expected if the length of the input\n",
    "    sequence is variable. It is usually preferable to use `hk.dynamic_unroll`.\n",
    "\n",
    "    Args:\n",
    "        params: the parameters of the RNN\n",
    "        initial_rnn_state: the initial state of the RNN\n",
    "        inputs: the data passed in input to the RNN\n",
    "        training: sets teacher forcing to True or False\n",
    "    \n",
    "    Returns:\n",
    "        The next value predictions.\n",
    "    \"\"\"\n",
    "    if training:\n",
    "      teacher_forcing = TEACHER_FORCING\n",
    "    else:\n",
    "      teacher_forcing = False\n",
    "\n",
    "    predictions = []\n",
    "    current_state = initial_rnn_state\n",
    "\n",
    "    # Unroll the rnn loop by hand.\n",
    "    for t in range(len(inputs)):\n",
    "\n",
    "        # When teacher forcing, the input is always the actual previous value \n",
    "        # (from the ground truth). Otherwise, we will feed the model with the \n",
    "        # actual previous values during the warm start period only, and then \n",
    "        # input the previous predictions instead.\n",
    "        if teacher_forcing or t <= WARM_START:\n",
    "            input_ = inputs[t]\n",
    "        else:\n",
    "            input_ = prediction\n",
    "\n",
    "        ######################\n",
    "        #   YOUR CODE HERE   #\n",
    "        ######################\n",
    "        # Connect the RNN: we apply our cell to (input, state) pairs, and we \n",
    "        # expect it to return (output, next_state) pairs; note how the current\n",
    "        # state is updated with the next_state.\n",
    "        rnn_embedding, next_state = None, None\n",
    "\n",
    "        ### Solution\n",
    "        rnn_embedding, next_state = RNN_cell(params, input_, current_state)\n",
    "\n",
    "        assert rnn_embedding is not None, 'The RNN must return an output'\n",
    "\n",
    "        current_state = next_state\n",
    "\n",
    "        ######################\n",
    "        #   YOUR CODE HERE   #\n",
    "        ######################\n",
    "        # Create a linear mapping from RNN output to the target (scalar)\n",
    "        # using the decoder and decoder bias parameters\n",
    "        prediction = None\n",
    "\n",
    "        ### Solution\n",
    "        prediction = jnp.dot(rnn_embedding, params['decoder']) + (\n",
    "            params['decoder_bias'])\n",
    "\n",
    "        assert prediction is not None, ('Connect the rnn embeddings and the '\n",
    "                                        'decoder to get prediction')\n",
    "\n",
    "        predictions.append(prediction)\n",
    "\n",
    "    return jnp.stack(predictions)\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mkgw8zfWnji7"
   },
   "source": [
    "### Shape check\n",
    "\n",
    "Let's check that our RNN predictions returns the desired shape"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "eK1tHmMsnjrY"
   },
   "source": [
    "rnd_key, k1, k2 = random.split(rnd_key, 3)\n",
    "params = initialize_parameters(k1)\n",
    "initial_rnn_state = get_initial_rnn_state(k2)\n",
    "predictions = predict(params, initial_rnn_state, sin_t[:UNROLL_LENGTH], True)\n",
    "assert len(predictions) == UNROLL_LENGTH"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oz66p6l0LIuf"
   },
   "source": [
    "## Loss and weights update [EXERCISE]\n",
    "\n",
    "We update the parameters of the network using the **ADAM** optimizer. In this section you need to complete the `loss_fn` that must compute the loss as the mean squared error of the prediction and the targets. As before, once you are done debugging uncomment the `@jit` decorator to ensure the code runs at full speed.\n",
    "\n",
    "The `optimize` function uses this `loss_fn` function to compute the loss and the gradient of the loss w.r.t. the parameters of the network, and takes one step of gradient descent to reduce such loss. This is effectively one step of training."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "pCChzriiyEun"
   },
   "source": [
    "@jit\n",
    "def loss_fn(params, rnn_state, inputs, targets):\n",
    "    \"\"\"\n",
    "    Loss function is a simple mean squared error. \n",
    "\n",
    "    Args:\n",
    "        params: the parameters of the neural network\n",
    "        rnn_state: the state of the rnn\n",
    "        inputs: a sequence of inputs\n",
    "        targets: a sequence of expected outputs\n",
    "\n",
    "    Returns:\n",
    "        The mean squared error.\n",
    "    \"\"\"\n",
    "\n",
    "    ######################\n",
    "    #   YOUR CODE HERE   #\n",
    "    ######################\n",
    "    # Obtain the prediction given the current state and input data\n",
    "    # and compute the mean squared error with the targets.\n",
    "    predictions = None\n",
    "    mse = None\n",
    "\n",
    "    ###Solution\n",
    "    predictions = predict(params, rnn_state, inputs, True)\n",
    "    mse = jnp.mean((predictions - targets)**2)\n",
    "\n",
    "    assert predictions is not None, 'Get the RNN predictions'\n",
    "    assert mse is not None, ('Compute the mean squared error between the '\n",
    "                             'predictions and the target')\n",
    "    assert predictions.shape == targets.shape, (\n",
    "        f'The predictions should have the same shape as the input and target, '\n",
    "        f'but are respectively {predictions.shape}, {inputs.shape}, '\n",
    "        f'{targets.shape}')\n",
    "\n",
    "    return mse\n",
    "\n",
    "@jit\n",
    "def optimize(params, opt_state, rnn_state, inputs, targets):\n",
    "    \"\"\"\n",
    "    Updates the parameters and returns the loss values.\n",
    "\n",
    "    Args:\n",
    "        params: the parameters of the neural network\n",
    "        opt_state: the state of the optimizer\n",
    "        rnn_state: the state of the rnn\n",
    "        inputs: a sequence of inputs\n",
    "        targets: a sequence of expected outputs\n",
    "    \n",
    "    Returns:\n",
    "        The loss, the new optimizer state and the updated network parameters.\n",
    "    \"\"\"\n",
    "    loss_value, grads = jax.value_and_grad(loss_fn)(params, rnn_state, inputs,\n",
    "                                                    targets)\n",
    "    updates, opt_state = optimizer(grads, opt_state)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    return loss_value, opt_state, params"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GZXgQs7Gfr_t"
   },
   "source": [
    "## Training [EXERCISE]\n",
    "\n",
    "In this section we will finally use all the functions defined above to train the network.\n",
    "\n",
    "The following code prints an example of input and output data for an unroll length of 4. Note how the target at time $t$ is the input at time $t+1$. This is because we train the network to predict the next value in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ihH0r4fHp9Et"
   },
   "source": [
    "training_input = jnp.array(sin_t[:4])\n",
    "training_target = jnp.array(sin_t[1:5])\n",
    "\n",
    "print('Example of training sample')\n",
    "print('input-> ', training_input.ravel())\n",
    "print('target-> ', training_target.ravel())"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xhG5FDqYwV3u"
   },
   "source": [
    "At this point, all the functions and parameters have been defined. We can create the training loop, run it and visualize the output plot predictions"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "o8oBn7BznGX0"
   },
   "source": [
    "######################\n",
    "#   YOUR CODE HERE   #\n",
    "######################\n",
    "# Initialize parameters and get the initial state of the RNN\n",
    "params = None\n",
    "initial_rnn_state = None\n",
    "\n",
    "### Solution\n",
    "rnd_key, k1, k2 = random.split(rnd_key, 3)\n",
    "params = initialize_parameters(k1)\n",
    "initial_rnn_state = get_initial_rnn_state(k2)\n",
    "\n",
    "assert params is not None, 'Initialize params of the RNN.'\n",
    "assert initial_rnn_state is not None, 'Get the initial state of the RNN.'\n",
    "\n",
    "\n",
    "opt_init, optimizer = optax.adam(STEP_SIZE)\n",
    "opt_state = opt_init(params)\n",
    "\n",
    "losses = []\n",
    "for i in range(NUM_ITERATIONS+1):\n",
    "    # Training.\n",
    "    start = np.random.choice(range(len(sin_t) - UNROLL_LENGTH))\n",
    "    training_input = jnp.array(sin_t[start: start+UNROLL_LENGTH])\n",
    "    training_target = jnp.array(sin_t[start+1: start+UNROLL_LENGTH+1])\n",
    "    # Note: `optimize` calls prediction, computes the loss and updates the \n",
    "    # parameters of the network and the state of the optimizer.\n",
    "    loss_value, opt_state, params = optimize(params, opt_state, \n",
    "                                             initial_rnn_state, training_input, \n",
    "                                             training_target)\n",
    "    losses.append(loss_value.tolist())\n",
    "\n",
    "    # Full sequence generation and plotting.\n",
    "    if i % REPORTING_INTERVAL == 0:\n",
    "\n",
    "        # Generate the full sequence (from the first to the last element).\n",
    "        y_gen = predict(params, initial_rnn_state, sin_t[:-1], False)\n",
    "        sampling_loss = loss_fn(params, initial_rnn_state, y_gen, sin_t[1:])\n",
    "        plt.figure(figsize=(10, 5))\n",
    "\n",
    "        plt.title(f'Training Loss {loss_value.tolist():.3f}, sampling loss '\n",
    "                  f'{sampling_loss.tolist():.3f}, iteration {i}')\n",
    "\n",
    "        plt.plot(t[1:].ravel(), sin_t[1:].ravel(), c='blue',\n",
    "                 label='Ground truth',\n",
    "                 linestyle=':', lw=6)\n",
    "\n",
    "        if TEACHER_FORCING:\n",
    "          plt.plot(t[start: start+UNROLL_LENGTH].ravel(),\n",
    "                   training_input.ravel(),\n",
    "                   alpha=0.7, lw=5, c='green', label='Training input')\n",
    "        else:\n",
    "          plt.plot(t[start: start+WARM_START].ravel(),\n",
    "                   training_input[:WARM_START].ravel(),\n",
    "                   alpha=0.7, lw=5, c='green', label='Training input')\n",
    "        plt.plot(t[start+1: start+UNROLL_LENGTH+1].ravel(),\n",
    "                 training_target.ravel(),\n",
    "                 alpha=0.7, lw=2, c='black', label='Training target')\n",
    "\n",
    "        plt.plot(t[start: start+UNROLL_LENGTH].ravel(),\n",
    "                 predict(params, initial_rnn_state, training_input, True),\n",
    "                 alpha=0.7, lw=3, c='gold', label='Network training prediction')\n",
    "\n",
    "        plt.plot(t[1:].ravel(),\n",
    "                 y_gen,\n",
    "                 alpha=0.7, c='r', label='Network full generation')\n",
    "\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.title('Training Loss')\n",
    "_ = plt.plot(losses)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X8KP_mmvXOXm"
   },
   "source": [
    "Now try changing the hyperparameters and see how they affect the training (e.g., how accurately does the network learn to model the curve, how fast does it converge, etc, ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LGoa8_e9w4BP"
   },
   "source": [
    "## **What is worth trying/understanding here?**\n",
    "- Difference between teacher forcing and learning on own samples:\n",
    " - What are the pros and cons of teacher forcing?\n",
    " - Why is the model struggling to learn in one of the setups?\n",
    " - What is it that we actually care about when we model sequences? How can this be negatively affected by teacher forcing?\n",
    "- Effect of warm starting:\n",
    "  - How does warm starting affect our training when teacher forcing is disabled? Why?\n",
    "  - How does warm starting affect our sampling when teacher forcing is enabled? Why?\n",
    "- What happens if the structure of interest is much longer than the unroll length?\n",
    "\n",
    "Answers:\n",
    "\n",
    "- Teacher forcing makes BPTT much easier and works better in practice. Intuition is similar to imitation learning. Without teacher forcing it is very hard to learn because error tend to accumulate. But using teacher forcing you get very local structure, meaning that the model is less able to generalize and is less robust to small perturbations (i.e., errors) of the input.\n",
    "- No teacher forcing makes training very difficult.\n",
    "- Depending on what you want to model, this loss may be fine if you care about probabilities but not generating samples.\n",
    "- Warm starting simplifies the task at the beginning of training, which makes the optimization easier, but (different from teacher forcing) eventually reintroduces the full task by conditioning on previous predictions. The learned model is thus more robust to noise in its input, which reduces compounding errors and improves the quality of sampled predictions.\n",
    "- The model learns to predict sequences of length unroll_length and at sampling time is required to generalize to longer sequences. When used to sample much longer predictions than the unroll length, their accuracy degrades."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G8W24GsiYgqh"
   },
   "source": [
    "# Part II: Character-level language modeling\n",
    "\n",
    "**Goal**: Train a character level LSTM on text data - specifically Shakespeare's sonnets. \n",
    "\n",
    "**What is an LSTM**: An LSTM is an advanced variant of the RNN. As opposed to the vanilla RNN the output and the state are separate, which allows for much more flexibility in deciding what to store. The LSTM is also characterized by a more advanced mechanism to determine which parts of the input to store into the memory and what to potentially forget. *For an in-depth analysis of the differences between various kinds of RNNs, we recommend you to read [this excellent guide](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) by Chris Olah.*\n",
    "\n",
    "**Haiku**: In order to develop our network we will use [Haiku](https://github.com/deepmind/dm-haiku), which provides many common building blocks, including LSTMs! \n",
    "\n",
    "**This tutorial**: Throughout this section you will build the network, train an LSTM on a dataset of Shakespeare's sonnets and finally you will be able to see how the quality of the generated text improves over time as the network trains."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Qyn2XG6xYNg9",
    "cellView": "form"
   },
   "source": [
    "# @title Hyper-parameters\n",
    "BATCH_SIZE = 32           #@param {type:'integer'}\n",
    "SEQUENCE_LENGTH = 128     #@param {type:'integer'}\n",
    "HIDDEN_SIZE = 256         #@param {type:'integer'}\n",
    "SAMPLE_LENGTH = 128       #@param {type:'integer'}\n",
    "LEARNING_RATE = 1e-3      #@param {type:'number'}\n",
    "TRAINING_STEPS = 6500   #@param {type:'integer'}\n",
    "SAMPLING_INTERVAL = 200   #@param {type:'integer'}\n",
    "SEED = 43                 #@param {type:'integer'}\n",
    "NUM_CHARS = 128           #@param {type:'integer'}"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "6KsmvXBRoKVb",
    "cellView": "form"
   },
   "source": [
    "#@title Dataset loader with encoding and decoding functions { form-width: \"170px\" }\n",
    "Batch = Mapping[str, np.ndarray]\n",
    "\n",
    "def load(split: tfds.Split, *, batch_size: int,sequence_length: int):\n",
    "    '''Creates the Tiny Shakespeare dataset as a character modelling task.'''\n",
    "\n",
    "    def preprocess_fn(x: Mapping[str, tf.Tensor]) -> Mapping[str, tf.Tensor]:\n",
    "        x = x['text']\n",
    "        x = tf.strings.unicode_split(x, 'UTF-8')\n",
    "        x = tf.squeeze(tf.io.decode_raw(x, tf.uint8), axis=-1)\n",
    "        x = tf.cast(x, tf.int32)\n",
    "        return {'input': x[:-1], 'target': x[1:]}\n",
    "\n",
    "    ds = tfds.load(name='tiny_shakespeare', split=split)\n",
    "    ds = ds.map(preprocess_fn)\n",
    "    ds = ds.unbatch()\n",
    "    ds = ds.batch(sequence_length, drop_remainder=True)\n",
    "    ds = ds.shuffle(100)\n",
    "    ds = ds.repeat()\n",
    "    ds = ds.batch(batch_size)\n",
    "    ds = ds.map(lambda b: tf.nest.map_structure(tf.transpose, b))  # Time major.\n",
    "    return tfds.as_numpy(ds)\n",
    "\n",
    "def decode(x: np.ndarray) -> str:\n",
    "  return ''.join([chr(x) for x in x])\n",
    "\n",
    "def encode(x: str) -> np.ndarray:\n",
    "  return np.array([ord(s) for s in x])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lbWSumSyijD9"
   },
   "source": [
    "## Build the network with Haiku [EXERCISE]\n",
    "\n",
    "In this section you will need to use Haiku to build the network. To do so, you will make use of the `hk.DeepRNN` ([doc](https://dm-haiku.readthedocs.io/en/latest/api.html#haiku.DeepRNN)) module to wrap together several layers into a single RNN core.\n",
    "\n",
    "Implement the following architecture:\n",
    "\n",
    "- Convert the input to a one-hot representation with `NUM_CHARS` elements (hint: use a lambda function `lambda x: jax.nn.one_hot(...)`) [(doc)](https://github.com/google/jax/blob/6c8fc1b031275c85b02cb819c6caa5afa002fa1d/jax/_src/nn/functions.py#L261)\n",
    "- LSTM of `HIDDEN_SIZE` size ([doc](https://dm-haiku.readthedocs.io/en/latest/api.html?highlight=lstm#lstm))\n",
    "- ReLU (non-linearity) ([doc](https://jax.readthedocs.io/en/latest/_autosummary/jax.nn.relu.html))\n",
    "- LSTM of `HIDDEN_SIZE` size ([doc](https://dm-haiku.readthedocs.io/en/latest/api.html?highlight=lstm#lstm))\n",
    "- An two-layer MLP composed of a layer of `HIDDEN_SIZE` size with ReLU activation, followed by a linear (no activation) layer mapping to the final problem size (number of characters: `NUM_CHARS`) ([docstring](https://github.com/deepmind/dm-haiku/blob/300e6a40be31e35940f0725ae7ed3457b737a5a3/haiku/_src/nets/mlp.py#L38))"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "0dIfFSU7ihSM"
   },
   "source": [
    "def make_network() -> hk.RNNCore:\n",
    "    \"\"\"Define the RNN core.\"\"\"\n",
    "    ######################\n",
    "    #   YOUR CODE HERE   #\n",
    "    ######################\n",
    "    # Design the LSTM with Haiku\n",
    "    # following the architecture\n",
    "    # defined in the text above.\n",
    "    rnn_core = None\n",
    "\n",
    "    #### Solution:\n",
    "    rnn_core = hk.DeepRNN([\n",
    "        lambda x: jax.nn.one_hot(x, num_classes=NUM_CHARS),\n",
    "        hk.LSTM(HIDDEN_SIZE),\n",
    "        jax.nn.relu,\n",
    "        hk.LSTM(HIDDEN_SIZE),\n",
    "        hk.nets.MLP([HIDDEN_SIZE, NUM_CHARS]),\n",
    "    ])\n",
    "\n",
    "    assert rnn_core is not None, 'LSTM model should be initialized'\n",
    "\n",
    "    return rnn_core"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YRFvPUdekDXG"
   },
   "source": [
    "## Loss and Update [EXERCISE]\n",
    "As before, we update the parameters of the network using the **ADAM** optimizer. \n",
    "\n",
    "In this section you need to complete the `loss_fn`, which should compute the loss as the categorical cross-entropy of the prediction and the targets. The `optimize` function (which you don't need to modify) uses this `loss_fn` function to compute the loss and the gradient of the loss w.r.t. the parameters of the network, and takes one step of gradient descent to reduce such loss. This is effectively one step of training.\n",
    "\n",
    "As before, once you are done debugging uncomment the `@jit` decorator **to ensure the code runs at full speed**.\n",
    "\n",
    "### Categorical Cross-entropy Recap\n",
    "The categorial cross-entropy loss is computed as\n",
    "\n",
    "$$\\mathcal{L}_{\\text{xent}} = - \\mathbf{y} \\cdot \\log(\\mathbf{\\hat{y}})$$\n",
    "\n",
    "where $\\mathbf{y}$ is the true class probability and $\\mathbf{\\hat{y}}$ is the predicted class probability.\n",
    "\n",
    "**Example**\n",
    "\n",
    "Let's take as an example a classification problem with 3 classes. Assume our **target vector** for the current sample to be $[0, 1, 0]$ (i.e., the second class is the correct one, according to the ground truth). If our network's **predicted class probability** is $[0.2, 0.7, 0.1]$, the categorical cross-entropy loss for this prediction will be\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "  \\mathcal{L}_{\\text{xent}} =& - \\bigg( 0 \\cdot \\log(0.2) + 1 \\cdot \\log(0.7) + 0 \\cdot \\log(0.1) \\bigg)\\\\\n",
    "  =& - \\log(0.7) \\\\ \n",
    "  =& \\; 0.35\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "In this case, the network was fairly confident that the input belonged to the second class, which was correct, so the error is not very large.\n",
    "\n",
    "Let's see what would have happened if the network was mistakenly confident that the true class was another one instead. Suppose the predicted vector was $[0.8, 0.05, 0.15]$, i.e., that the network was assigning a very low probability to the true class. The loss in this case would have been $-\\log(0.05) = 3$, which is much higher than before."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "VZJg0m3SYqLm"
   },
   "source": [
    "def loss_fn(batch):\n",
    "    \"\"\"\n",
    "    Compute the categorical cross-entropy loss.\n",
    "\n",
    "    Unroll the network over a sequence of batched inputs & targets, and\n",
    "    compute the categorical cross-entropy loss.\n",
    "\n",
    "    Args:\n",
    "        logits: the logits from the network\n",
    "        batch: a batch of input and target data\n",
    "\n",
    "    Returns:\n",
    "        The categorical cross-entropy loss.\n",
    "    \"\"\"\n",
    "    sequence_length, batch_size = batch['input'].shape\n",
    "\n",
    "    ######################\n",
    "    #   YOUR CODE HERE   #\n",
    "    ######################\n",
    "    # Obtain the prediction by computing the log_softmax on logits\n",
    "    # Convert the log probabilities to one hot labels\n",
    "    # and compute the categorical cross-entropy loss\n",
    "    loss = None\n",
    "\n",
    "    ### Solution\n",
    "    # Unroll the network through time.\n",
    "    rnn_core =  make_network()\n",
    "    initial_state = rnn_core.initial_state(BATCH_SIZE)\n",
    "    logits, _ = hk.dynamic_unroll(rnn_core, batch['input'], initial_state)\n",
    "\n",
    "    log_probs = jax.nn.log_softmax(logits)\n",
    "    num_classes = logits.shape[-1]\n",
    "    one_hot_labels = jax.nn.one_hot(batch['target'], num_classes=num_classes)\n",
    "    # Categorical cross entropy  # TODO: use mean over the axes?\n",
    "    loss = -jnp.sum(one_hot_labels * log_probs) / (sequence_length * batch_size)\n",
    "\n",
    "    assert loss is not None, 'Loss should be computed'\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "@jit\n",
    "def optimize(params, opt_state, batch):\n",
    "    \"\"\"\n",
    "    Applies an update to the parameters.\n",
    "\n",
    "    Args:\n",
    "        params: the parameters of the neural network\n",
    "        opt_state: the state of the optimizer\n",
    "        batch:  batch of input and target data\n",
    "\n",
    "    Returns:\n",
    "        The new optimizer state and the updated network parameters.\n",
    "    \"\"\"\n",
    "    ######################\n",
    "    #   YOUR CODE HERE   #\n",
    "    ######################\n",
    "    # Similar to the RNN update function\n",
    "    # - initialize the Adam optimizer\n",
    "    # - apply grad function to loss\n",
    "    # - compute params updates\n",
    "    # - apply updates on params\n",
    "    new_params = None\n",
    "    new_opt_state = None\n",
    "\n",
    "    ### Solution\n",
    "    # Note that since JAX is stateless (the state is passed explicitly to each \n",
    "    # operation) here we create another optimizer function for simplicity, \n",
    "    # instead of passing the one we create when we first initialize the state\n",
    "    # of the optimizer.\n",
    "    optimizer = optax.adam(LEARNING_RATE)\n",
    "    _, transformed_loss = hk.without_apply_rng(hk.transform(loss_fn))\n",
    "    gradients = jax.grad(transformed_loss)(params, batch)\n",
    "    # Transform & update stats by Adam and update the parameters.\n",
    "    updates, new_opt_state = optimizer.update(gradients, opt_state)\n",
    "    new_params = optax.apply_updates(params, updates)\n",
    "\n",
    "    assert new_opt_state is not None, 'Compute the updated optimizer\\'s state.'\n",
    "    assert new_params is not None, 'Compute the updated parameters.'\n",
    "\n",
    "    return new_params, new_opt_state"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rwOqO0to_jMG"
   },
   "source": [
    "## Sampling function\n",
    "\n",
    "This function draws samples from the model, given an initial context."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "D_n_UPE3Y2J5"
   },
   "source": [
    "def sample_fn(rng_key: jnp.ndarray, context: jnp.ndarray, sample_length: int):\n",
    "    \"\"\"\n",
    "    Draws samples from the model, given an initial context.\n",
    "\n",
    "    Note: this function is impure; we will hk.transform() it (and jit it) in the\n",
    "    training loop.\n",
    "\n",
    "    Args:\n",
    "        rng_key: random key\n",
    "        context: initial context given to the model\n",
    "        sample_length: length of the sample\n",
    "\n",
    "    Returns:\n",
    "        the generated tokens\n",
    "    \"\"\"\n",
    "    \n",
    "    assert context.ndim == 1  # Sequence only, no batch\n",
    "\n",
    "    def body_fn(t, values):\n",
    "        tokens, state, rng_key = values\n",
    "        token = tokens[t]\n",
    "        next_logits, next_state = rnn_core(token, state)\n",
    "        rng_key, k1 = jax.random.split(rng_key)\n",
    "        next_token = jax.random.categorical(k1, next_logits, axis=-1)\n",
    "        new_tokens = ops.index_update(tokens, ops.index[t + 1], next_token)\n",
    "        return new_tokens, next_state, rng_key\n",
    "\n",
    "    # Unroll over context (initial prompt).\n",
    "    rnn_core =  make_network()\n",
    "    initial_state = rnn_core.initial_state(None)  # no batch here!\n",
    "    logits, state = hk.dynamic_unroll(rnn_core, context, initial_state)\n",
    "\n",
    "    # Sample the first continuation token and initialize the output array.\n",
    "    rng_key, k1 = jax.random.split(rng_key)\n",
    "    first_token = jax.random.categorical(k1, logits[-1])\n",
    "    tokens = np.zeros(sample_length, dtype=np.int32)\n",
    "    tokens = ops.index_update(tokens, ops.index[0], first_token)\n",
    "\n",
    "    # Sample the other tokens in a loop.\n",
    "    initial_values = tokens, state, rng_key\n",
    "    new_tokens, _, _ = lax.fori_loop(0, sample_length, body_fn, initial_values)\n",
    "\n",
    "    return new_tokens"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5KzeOViMi9zg"
   },
   "source": [
    "## Train and generate language\n",
    "Finally, this training loop puts everything together. The network is trained for a fixed number of `TRAINING_STEPS` and is evaluated at fixed intervals. During the evaluation, given a *context* sentence provided as input, the network tries to generate some text to complete that prompt.\n",
    "\n",
    "As you can see, for the first few iterations the model generates nonsensical sentences, but after some training you can observe that the model is actually learning to reproduce text that resembles Shakespeare's sonnets.\n",
    "\n",
    "**Important:** make sure you uncommented the `@jit` decorators in the previous\n",
    "code to enjoy accelerated training. Training should take less than 7 minutes.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "p4xF1fUko7BC"
   },
   "source": [
    "%%time\n",
    "# Note that since we use Haiku, we use their PRNG instead of the one from JAX.\n",
    "rng = hk.PRNGSequence(SEED)\n",
    "\n",
    "train_data = load(tfds.Split.TRAIN, batch_size=BATCH_SIZE,\n",
    "                  sequence_length=SEQUENCE_LENGTH)\n",
    "train_data = iter(train_data)\n",
    "\n",
    "# Out network doesn't have a nondifferentiable state (the inner states of the \n",
    "# LSTMs is differentiable), so we don't need to use `hk.transform_with_state` \n",
    "# and can use the typical `hk.transform` instead. Further, our network does not\n",
    "# have stochastic operations, so we can wrap it in a `without_apply_rng` call.\n",
    "# Note that here we transform `loss_fn` as a way to initialize the network\n",
    "# parameters a few lines below.\n",
    "transformed_loss = hk.without_apply_rng(hk.transform(loss_fn))\n",
    "_, sample = hk.without_apply_rng(hk.transform(sample_fn))\n",
    "sample = jax.jit(sample, static_argnums=[3])  # sample_length is static\n",
    "\n",
    "# Setup optimizer.\n",
    "params = transformed_loss.init(next(rng), next(train_data))\n",
    "# Similar to what we do for the loss, here we create the optimizer just to get\n",
    "# the initial optimizer state.\n",
    "optimizer = optax.adam(LEARNING_RATE)\n",
    "opt_state = optimizer.init(params)\n",
    "\n",
    "BOLD = '\\033[1m'\n",
    "END =  '\\033[0m'\n",
    "SEP = '\\n------------------------------\\n'\n",
    "for step in range(TRAINING_STEPS):\n",
    "    train_batch = next(train_data)  # includes input and target\n",
    "    \n",
    "    # Run one step of training and update the parameters.\n",
    "    params, opt_state = optimize(params, opt_state, train_batch)\n",
    "\n",
    "    if step % SAMPLING_INTERVAL == 0:\n",
    "        # Use the input text of the batch as context.\n",
    "        context = train_batch['input'][:, 0]  # drop the batch\n",
    "        rng_key = next(rng)\n",
    "        # Sample generated text given context.\n",
    "        samples = sample(params, rng_key, context, SAMPLE_LENGTH)\n",
    "        # Decode context and samples to actual sentences.\n",
    "        prompt = decode(context)\n",
    "        continuation = decode(samples)\n",
    "\n",
    "        print(f'{BOLD}>>> Prompt (from the dataset):{SEP}{END}\\n{prompt}\\n')\n",
    "        print(f'{BOLD}Continuation (from the network):{END}\\n{continuation}\\n')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fplu3Y4HBSMt"
   },
   "source": [
    "## **What is worth trying/understanding here?**\n",
    "- After how many iterations the model is able to learn a sentence which makes sense?\n",
    "- Why the first generated sentences do not make sense and what pattern can you see?\n",
    "- How is the sample length parameters affecting the results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RjLisc_tNNKd"
   },
   "source": [
    "# Part III: Topic modeling with Non-negative Matrix Factorization\n",
    "\n",
    "**Goal**: Extract topics from a set of documents exploiting *matrix factorization*.\n",
    "\n",
    "**What is topic modeling**: *Topic modeling* is a Natural Language Processing task whose aim is to discover the abstract *topics* that occur in a set of documents. Intuitively the idea is that, depending on the topic discussed in a document or a set of documents, specific words will appear more or less frequently. For instance, 'dog' and 'bone' will appear more often in documents about dogs, while 'cat' and 'meow' will appear in documents about cats. Common words, like 'the' and 'is', will tend to appear approximately uniformely in all kinds of documents so are not very informative to tell the documents apart. We can exploit this to cluster the documents in groups, according to their topic.\n",
    "\n",
    "**Credits:** this code is an adaptation of [this fast.ai tutorial](https://nbviewer.jupyter.org/github/fastai/numerical-linear-algebra/blob/master/nbs/2.%20Topic%20Modeling%20with%20NMF%20and%20SVD.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TANdj0b-DRXB"
   },
   "source": [
    "## Load dataset\n",
    "\n",
    "In this section, we will use the well-known benchmark dataset **20newsgroups** to model the main topics discussed in a group of conversations. Newsgroups are discussion groups on Usenet, which was popular in the 80s and 90s before the web really took off. This dataset includes 18,000 newsgroups posts on 20 different topics."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "5ghf_ESxFyNd"
   },
   "source": [
    "categories = ['rec.motorcycles', 'sci.med', 'comp.graphics', 'sci.space']\n",
    "remove = ('headers', 'footers', 'quotes')\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', categories=categories, remove=remove)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "idL_zP9hE1Kx"
   },
   "source": [
    "# Print three random documents.\n",
    "print('\\n\\n-----\\n'.join(newsgroups_train.data[:3]))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "bAYlZGHbFFdk"
   },
   "source": [
    "# Print the category of the previous documents.\n",
    "np.array(newsgroups_train.target_names)[newsgroups_train.target[:3]]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Txw-ngsTFQvq"
   },
   "source": [
    "## Document discrete representations\n",
    "\n",
    "In order to represent the documents as a **term-document matrix**, we need to transform unstructured data (text) to structured data.\n",
    "\n",
    "This can be performed by extracting count and/or frequencies of words in each documents.\n",
    "\n",
    "- The **Bag of Words (BoW)** model is the simplest way to represent text in numerical form. As the name suggests (nomen omen!), given a vocabulary of words we can represent any sentence as a *bag of words vector* (i.e., a vector of numbers) using the *counts*, i.e. we count how many times each word in the dictionary appears in the documents.\n",
    "\n",
    "- The **TF-IDF** (Term Frequency - Inverse Document Frequency) representation model is similar, but accounts for both the frequencies of words and their importance. For example, if a words appears in all the documents, it is not very important, as it does not allow us to differentiate documents.\n",
    "\n",
    "We will use **TF-IDF** representations, which we will extract using the [sklearn](https://scikit-learn.org/) library. \n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "BbYELvNMF0VE"
   },
   "source": [
    "# Initialize the TF-IDF representation model.\n",
    "tf_idf = TfidfVectorizer(stop_words='english', min_df=10)\n",
    "# Apply TF-IDF representation to the documents.\n",
    "X_train = tf_idf.fit_transform(newsgroups_train['data'])\n",
    "X_train = X_train.todense()\n",
    "X_train = jnp.array(X_train)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "eP0RgsnTHAYz"
   },
   "source": [
    "X_train.shape"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ROTuT5neHIfx"
   },
   "source": [
    "# Each column of the representation matrix corresponds to a word.\n",
    "vocab = np.array(tf_idf.get_feature_names())\n",
    "print(vocab)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "af6jFeZhHlx-"
   },
   "source": [
    "## Non-Negative Matrix Factorization [EXERCISE]\n",
    "\n",
    "**Non-Negative Matrix Factorization** is a statistical method to reduce the dimension of the input corpora. It uses the [factor analysis](https://en.wikipedia.org/wiki/Factor_analysis) method to provide comparatively less weight to the words with least coherence.\n",
    "\n",
    "To reason on a practical example, assume we have an input matrix $V$ of shape $m \\times n$. This method factorizes $V$ into two matrices $W$ and $H$, such that the dimension of $W$ is $m \\times k$ and that of $H$ is $k \\times n$. In our case, $V$ represent the *term-document matrix* (the frequency of the terms that occur in the collection of documents), each row of the matrix $H$ is a word embedding and each column of the matrix $W$ represent the weight of each word in each of the sentences (i.e., the semantic relation of words with each sentence). \n",
    "\n",
    "![NMF](https://miro.medium.com/max/400/0*uz3OkHMgjAH2Yc40.png)\n",
    "\n",
    "The assumption is that all the entries of $W$ and $H$ are positive, since all the entries of $V$ are positive.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "C8cUIxDfF2Jw"
   },
   "source": [
    "def initialize_parameters(rnd_key, X_train, k):\n",
    "    \"\"\"\n",
    "    This function will initialize and returns the Vanilla RNN parameters.\n",
    "    \n",
    "    Args:\n",
    "        rnd_key: random key\n",
    "        X_train: term-document matrix\n",
    "        k: hidden dimension\n",
    "\n",
    "    Returns:\n",
    "        A list of parameters [W, V, bias, decoder, decoder_bias].\n",
    "    \"\"\"\n",
    "    ######################\n",
    "    #   YOUR CODE HERE   #\n",
    "    ######################\n",
    "    # Initialize the NNMF parameters (W and H) using normal distribution,\n",
    "    # with the correct shapes\n",
    "    # (mean zero and unit standard deviation) and correct shapes\"\n",
    "    \n",
    "    m, n = X_train.shape\n",
    "    k1, k2 = random.split(rnd_key)\n",
    "    params = None\n",
    "\n",
    "    #### Solution:\n",
    "    params = {\n",
    "        'W' : normal()(k1, (m, k)),\n",
    "        'H' : normal()(k2, (k, n)),\n",
    "    }\n",
    "\n",
    "    assert params is not None, 'Params should be initialized'\n",
    "\n",
    "    return params\n",
    "    "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N2K4GIGPK9cK"
   },
   "source": [
    "## Frobenius Norm\n",
    "\n",
    "A way to perform NMF is by using the Frobenius norm, which is defined by the square root of the sum of the absolute squares of its elements. \n",
    "\n",
    "$$ ||A||_F = \\sqrt{\\sum_{i=1}^m \\sum_{j=1}^n | a_{i,j}|^2}$$\n",
    "\n",
    "We want to minimize the Frobenious norm of\n",
    "\n",
    "$$ V - WH $$\n",
    "\n",
    "however, we also want to penalize the matrix $W$ and $H$ when they are negative. Thus, we will add a specific penalty function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nisF3ArtKbvs"
   },
   "source": [
    "### Penalty [EXERCISE]"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "9lQdepL7K16j"
   },
   "source": [
    "def compute_penalty(matrix):\n",
    "    \"\"\"\n",
    "    Return the average of the square of the negative values in the matrix.\n",
    "\n",
    "    Args:\n",
    "        matrix: the matrix on which we want to compute the penality\n",
    "\n",
    "    Returns:\n",
    "        The penalty\n",
    "\n",
    "    \"\"\"\n",
    "    # Equivalent to `(matrix[matrix < 0]**2).mean()`\n",
    "    return jnp.power(jnp.clip(matrix, a_max=0.), 2).mean()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DWEwjHRxMvF3"
   },
   "source": [
    "Let's see this function in action: the \"more negative\" the matrix is, the higher the penalty will be"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "SJR6VWtxMpmj"
   },
   "source": [
    "A = jnp.array([[0, 1, 4], [5, 6, 7]]) # positive\n",
    "B = jnp.array([[0, 1, -4], [5, 6, 7]]) # some negative elments\n",
    "C = jnp.array([[0, 1, 4], [-5, -6, -7]]) # more negative elements\n",
    "\n",
    "print(\"A\", compute_penalty(A))\n",
    "print(\"B\", compute_penalty(B))\n",
    "print(\"C\", compute_penalty(C))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7wzoaA94Lz_k"
   },
   "source": [
    "We can now use this into our general loss function"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "GCZ0dNYyF5Fe"
   },
   "source": [
    "def loss_fn(params, V):\n",
    "    \"\"\"\n",
    "    Compute the Frobenius norm factorization loss\n",
    "    \n",
    "    The Frobenius norm of the matrix V and its matrix factorization (W*H),\n",
    "    plus the penalty scaled by the regularization parameter.\n",
    "    \n",
    "    Args:\n",
    "        params: params to optimize (W and H)\n",
    "        V: matrix\n",
    "    Returns:\n",
    "        The Frobenius norm factorization loss.\n",
    "    \"\"\"\n",
    "    ######################\n",
    "    #   YOUR CODE HERE   #\n",
    "    ######################\n",
    "    # Compute the loss and sum the regularization.\n",
    "    # Combine the frobenious norm with the penality for both matrices,\n",
    "    # since both have to be positive.\n",
    "    # (weight the penality by the\n",
    "    # regularization parameter)\n",
    "    loss = None\n",
    "    regularization_param=1e6\n",
    "\n",
    "    ### Solution\n",
    "    penalty = (compute_penalty(params['W']) + compute_penalty(params['H'])) * regularization_param\n",
    "    loss = jnp.linalg.norm(V - jnp.matmul(params['W'], params['H'])) + penalty\n",
    "\n",
    "    assert loss is not None, 'Loss should be computed'\n",
    "\n",
    "    return loss"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1iOseGnkNKQ0"
   },
   "source": [
    "## Parameters update function\n",
    "As in the previous sections, the `optimize` function (which you don't need to modify) uses the `loss_fn` function to compute the loss and the gradient of the loss w.r.t. the parameters of the network, and takes one step of gradient descent to reduce such loss. This is effectively one step of training."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ZhA8TBHhF-OT"
   },
   "source": [
    "@jit\n",
    "def optimize(params, opt_state, x):\n",
    "    loss_value, grads = jax.value_and_grad(loss_fn)(params, x)\n",
    "    updates, opt_state = opt_update(grads, opt_state)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    return loss_value, opt_state, params"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LuzpN5-nNMfq"
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "3r8eYcrJF5u8"
   },
   "source": [
    "rnd_key = random.PRNGKey(1)\n",
    "rnd_key, k1, k2 = random.split(rnd_key, 3)\n",
    "params = initialize_parameters(k1, X_train, 5)\n",
    "opt_init, opt_update = optax.adam(0.01)\n",
    "opt_state = opt_init(params)\n",
    "for _ in range(0, 500):\n",
    "    loss_value, opt_state, params = optimize(params, opt_state, X_train)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JaBFzdAyNUgU"
   },
   "source": [
    "## Visualize output topics"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ZceLfs95GfN7"
   },
   "source": [
    "# The number of most-common words that we want to visualize.\n",
    "num_top_words = 5  #@param {type:\"integer\"}\n",
    "\n",
    "def show_topics(H_matrix):\n",
    "    \"\"\"\n",
    "    Shows the topic coming from the H matrix. \n",
    "    To be used after we have trained our NMF model.\n",
    "\n",
    "    Args:\n",
    "        H_matrix: the matrix that contains the topics\n",
    "\n",
    "    Returns: \n",
    "        A list of lists with the topic words for each topic\n",
    "    \"\"\"\n",
    "    top_words = lambda t: [vocab[i] for i in np.argsort(t)[:-num_top_words-1:-1]]\n",
    "    topic_words = ([top_words(t) for t in H_matrix])\n",
    "    return [' '.join(t) for t in topic_words]\n",
    "\n",
    "show_topics(params['H'])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CmWBhBGilOjc"
   },
   "source": [
    "# Part IV: Text classification with neural networks\n",
    "\n",
    "**Goal**: Perform text classification exploiting traditional and advanced representation models and assess their impact on classification performance.\n",
    "\n",
    "**What is text classification**: *Text classification* is the most basic task of Natural Language Processing. The goal is to classify textual data as belonging to one (yes/no) or more categories.\n",
    "\n",
    "In this section, we take the specific task of **Sentiment Analysis** applied to reviews. Sentiment analysis, also called *opinion mining*, is the field of study that analyzes peoples opinions, sentiments, evaluations, appraisals, attitudes, and emotions towards entities such as products, services, organizations, individuals, issues, events, topics, and their attributes. Commonly, Sentiment Analysis is performed as a classification problem where a text should be classified as being *positive* or *negative*.\n",
    "\n",
    "To address this task, we will investigate various representation models with a simple neural network. The goal of this section is to show the impact of the different representations on the classification accuracy.\n",
    "\n",
    "The code has been adapted from [[link](https://github.com/google/jax/blob/master/docs/notebooks/neural_network_with_tfds_data.ipynb)]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LBrETGNk9L15"
   },
   "source": [
    "## Load dataset\n",
    "\n",
    "Here we load the *IMDB reviews* dataset for binary sentiment classification. The training set is composed of 25,000 highly movie reviews."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Xxp8umfhDyj3"
   },
   "source": [
    "# Load and preprocess the data.\n",
    "train_data, test_data = tfds.load(name='imdb_reviews', split=['train', 'test'], \n",
    "                                  batch_size=-1, as_supervised=True)\n",
    "train_examples, train_labels = tfds.as_numpy(train_data)\n",
    "test_examples, test_labels = tfds.as_numpy(test_data)\n",
    "train_examples = list(map(lambda x : x.decode('utf-8') , train_examples))\n",
    "test_examples = list(map(lambda x : x.decode('utf-8') , test_examples))\n",
    "\n",
    "def batch(X, y, batch_size=32):\n",
    "    \"\"\"\n",
    "    This function will allow to select batches of training data.\n",
    "\n",
    "    Args:\n",
    "        X: train raw data\n",
    "        y: train labels\n",
    "        batch_size: size of the batch\n",
    "    \"\"\"\n",
    "    total_len = len(X)\n",
    "    for ndx in range(0, total_len, batch_size):\n",
    "        yield (X[ndx:min(ndx + batch_size, total_len)], \n",
    "               y[ndx:min(ndx + batch_size, total_len)])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZVN_Q04uge_D"
   },
   "source": [
    "## Initialize Variables"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "BcVSkSml0ovi"
   },
   "source": [
    "PARAM_SCALE = 0.1 #@param {type:'number'}\n",
    "STEP_SIZE = 0.01 #@param {type:'number'}\n",
    "NUM_EPOCHS = 10 #@param {type:'integer'}\n",
    "BATCH_SIZE = 128 #@param {type:'integer'}\n",
    "N_TARGETS = 2 #@param {type:'integer'}"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uQ3PUS3G3SCe"
   },
   "source": [
    "## Traditional Representation: TF-IDF [EXERCISE]\n",
    "\n",
    "Similar to what we've previously seen, we use TF-IDF representation for the discrete representation of textual data."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "DgDHhYs3n8J1"
   },
   "source": [
    "def get_discrete_data(train_examples,test_examples):\n",
    "\n",
    "    ######################\n",
    "    #   YOUR CODE HERE   #\n",
    "    ######################\n",
    "    # Transform test and train data with TF-IDF\n",
    "    # with min document frequency set to 80\n",
    "    # transform arrays to dense arrays \n",
    "    X_train = None\n",
    "    X_test = None\n",
    "\n",
    "    ### Solution\n",
    "    tf_idf = TfidfVectorizer(min_df=80)\n",
    "    X_train = tf_idf.fit_transform(train_examples)\n",
    "    X_test = tf_idf.transform(test_examples)\n",
    "    X_train = X_train.todense()\n",
    "    X_test = X_test.todense()\n",
    "\n",
    "    assert X_train is not None, 'Data should be transformed with TF-IDF vectorizer'\n",
    "\n",
    "    return X_train, X_test"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-u-w9Of0NeF0"
   },
   "source": [
    "## Initialize parameters [EXERCISE]"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "cNIDKj9JnBOk"
   },
   "source": [
    "def random_layer_params(m, n, key, scale=1e-2):\n",
    "    w_key, b_key = random.split(key)\n",
    "    return (scale * random.normal(w_key, (n, m)),\n",
    "            scale * random.normal(b_key, (n,)))\n",
    "\n",
    "def init_network_params(sizes, rnd_key):\n",
    "    \"\"\"\n",
    "    Initialize neural network parameters. \n",
    "\n",
    "    Args:\n",
    "        sizes: layers' size\n",
    "        key: random key\n",
    "\n",
    "    Returns:\n",
    "        The initialized parameters.\n",
    "    \"\"\"\n",
    "    keys = random.split(rnd_key, len(sizes))\n",
    "    return [random_layer_params(m, n, k) \n",
    "            for m, n, k in zip(sizes[:-1], sizes[1:], keys)]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "IzSRCR6Pnb8-"
   },
   "source": [
    "def predict(params, sentence):\n",
    "    \"\"\"\n",
    "    Predict class given a sentence. \n",
    "\n",
    "    Args:\n",
    "        params: the parameters of the neural network\n",
    "        sentence: input data\n",
    "\n",
    "    Returns:\n",
    "        The predictions of the network\n",
    "    \"\"\"\n",
    "    activations = sentence\n",
    "\n",
    "    ######################\n",
    "    #   YOUR CODE HERE   #\n",
    "    ######################\n",
    "    # For each layer in the neural network\n",
    "    # compute the basic MLP (with weights and bias)\n",
    "    # using relu as activation function.\n",
    "    # Pay attention to the last layer\n",
    "\n",
    "    output_last_layer = None\n",
    "\n",
    "    ### Solution\n",
    "    for w, b in params[:-1]:\n",
    "      outputs = jnp.dot(w, activations) + b\n",
    "      activations = jax.nn.relu(outputs)\n",
    "    \n",
    "    final_w, final_b = params[-1]\n",
    "    output_last_layer = jnp.dot(final_w, activations) + final_b\n",
    "\n",
    "    assert output_last_layer is not None, 'Design MLP architecture'\n",
    "\n",
    "    return jax.nn.log_softmax(output_last_layer)\n",
    "\n",
    "# We use vmap to make the prediction batched.\n",
    "batched_predict = vmap(predict, in_axes=(None, 0))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "131LNBNknjVl"
   },
   "source": [
    " def accuracy(params, x, y):\n",
    "    \"\"\"\n",
    "    Compute accuracy measure.\n",
    "\n",
    "    Args:\n",
    "        x: input data\n",
    "        y: target data\n",
    "\n",
    "    Returns:\n",
    "        The classification accuracy\n",
    "    \"\"\"\n",
    "\n",
    "    ######################\n",
    "    #   YOUR CODE HERE   #\n",
    "    ######################\n",
    "    # Compute the accuracy.\n",
    "    #\n",
    "    # Reminder: the accuracy is \n",
    "    # the average number of correctly \n",
    "    # predicted classes.\n",
    "    #\n",
    "    # Hint: Use batched_predict to get the\n",
    "    # classes ids predicted by the network,\n",
    "    # and compare them with the target\n",
    "    # classes from the ground truth.\n",
    "    \n",
    "    accuracy = None\n",
    "    \n",
    "    ### Solution\n",
    "    target_class = jnp.argmax(y, axis=1)\n",
    "    predicted_class = jnp.argmax(batched_predict(params, x), axis=1)\n",
    "    accuracy = jnp.mean(predicted_class == target_class)\n",
    "\n",
    "    assert accuracy is not None, 'Accuracy should be computed'\n",
    "\n",
    "    return accuracy"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "76P8z-a_N106"
   },
   "source": [
    "## Loss and update"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "dFEeG4kJN1Ez"
   },
   "source": [
    "def loss(params, x, y):\n",
    "    \"\"\"\n",
    "    Loss function. \n",
    "\n",
    "    Args:\n",
    "        params: the parameters of the neural network\n",
    "        x: input data\n",
    "        y: target data (class)\n",
    "\n",
    "    Returns:\n",
    "        The computed loss\n",
    "    \"\"\"\n",
    "    preds = batched_predict(params, x)\n",
    "    return -jnp.mean(preds * y)\n",
    "\n",
    "@jit\n",
    "def update(params, x, y):\n",
    "    \"\"\"\n",
    "    The optimization function. \n",
    "\n",
    "    Args:\n",
    "        params: the parameters of the neural network\n",
    "        x: input data\n",
    "        y: target data (class)\n",
    "    \n",
    "    Returns:\n",
    "        The updated parameters\n",
    "    \"\"\"\n",
    "    loss_value, grads = jax.value_and_grad(loss)(params, x, y)\n",
    "    return [(w - STEP_SIZE * dw, b - STEP_SIZE * db)\n",
    "            for (w, b), (dw, db) in zip(params, grads)], loss_value"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ndza_KYdP17L"
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "uzEpDD5kT7Tb"
   },
   "source": [
    "def train_loop(train_examples, test_examples, train_labels, test_labels, \n",
    "               layer_sizes=None):\n",
    "    # Visualize learning progress with a bar.\n",
    "    pbar = tqdm(total=NUM_EPOCHS, position=0, leave=True)\n",
    "\n",
    "    X_train, X_test = get_discrete_data(train_examples, test_examples)\n",
    "    if layer_sizes is None:\n",
    "        layer_sizes = [X_train.shape[1], 256,  2]\n",
    "    params = init_network_params(layer_sizes, random.PRNGKey(0))\n",
    "\n",
    "    # Transform target data to one-hot representation.\n",
    "    y = jax.nn.one_hot(train_labels, N_TARGETS)\n",
    "    y_test = jax.nn.one_hot(test_labels, N_TARGETS)\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        # Update parameters for each batch.\n",
    "        for x_t, y_t in batch(X_train, y):\n",
    "            params, loss_value = update(params, x_t, y_t)\n",
    "        pbar.update(1)\n",
    "        \n",
    "        # Compute accuracy on training and test.\n",
    "        train_acc = accuracy(params, X_train, y)\n",
    "        test_acc = accuracy(params, X_test, y_test)\n",
    "\n",
    "        pbar.set_description('Loss value is {0:.2f}, training accuracy is '\n",
    "                            '{1:.5}, test accuracy is {2:.5}'.format(loss_value,\n",
    "                                                                    train_acc,\n",
    "                                                                    test_acc))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "K9a6qHWRqkxd"
   },
   "source": [
    "train_loop(train_examples, test_examples, train_labels, test_labels)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AJIgaUQK4PHh"
   },
   "source": [
    "## Preprocessing + TF-IDF\n",
    "\n",
    "Text preprocessing is traditionally an important step for NLP tasks. It transforms text into a more digestible form so that machine learning algorithms can perform better. For example:\n",
    "- it removes 'not meaningful words' such as a,an,the,for, etc. (so-called *stopwords*)\n",
    "- lowercase everything\n",
    "- remove symbols\n",
    "- remove or process hashtags, mentions or emoticons (particularly important for social media text)\n",
    "\n",
    "We expect that 'cleaning' the text will help us on obtaining better performance."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "NOGdOddT0wQ1"
   },
   "source": [
    "# Preprocessing functions\n",
    "def split_hashtag(tag):\n",
    "    pattern = re.compile(r'[A-Z][a-z]+|\\d+|[A-Z]+(?![a-z])')\n",
    "    return pattern.findall(tag)\n",
    "\n",
    "\n",
    "def quick_preprocessing(sentence, language, process_urls=False, \n",
    "                        process_mentions=False, process_emoticon=False,\n",
    "                        split_hashtags=False):\n",
    "    \"\"\"\n",
    "    A quick preprocessing function that removes unwanted tokens from the text\n",
    "\n",
    "    Args:\n",
    "        sentence: the sentence to be preprocessed\n",
    "        language: the language the sentence is in\n",
    "        process_urls: whether to process URLs\n",
    "        process_mentions: whether to process mentions\n",
    "        process_emoticon: whether to process emoticons\n",
    "        split_hashtags: whether to split hashtags\n",
    "\n",
    "    Returns:\n",
    "        The modified sentence given the preprocessing configuration\n",
    "\n",
    "    \"\"\"\n",
    "    url_re = (r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f]'\n",
    "              r'[0-9a-f]))+')\n",
    "    mention_re = r'(?:@[\\w_]+)'\n",
    "    emoticons_re = r'(?::|;|=)(?:-)?(?:\\)|\\(|D|P)'\n",
    "\n",
    "    if process_urls:\n",
    "        sentence = re.sub(url_re, 'TAGurl', sentence, flags=re.MULTILINE)\n",
    "    if process_mentions:\n",
    "        sentence = re.sub(mention_re, 'TAGmention', sentence,\n",
    "                          flags=re.MULTILINE)\n",
    "    if process_emoticon:\n",
    "        sentence = re.sub(emoticons_re, 'TAGemoticon', sentence,\n",
    "                          flags=re.MULTILINE)\n",
    "                 \n",
    "    if split_hashtags:\n",
    "        hashtag_list = list({tag.strip('#') for tag in sentence.split() \n",
    "                             if tag.startswith('#')})\n",
    "        hashtag_split_list = [' '.join(split_hashtag(h)) for h in hashtag_list]\n",
    "        for i in range(len(hashtag_list)):\n",
    "            sentence = sentence.replace(hashtag_list[i],\n",
    "                                        ' ' + hashtag_split_list[i] + ' ')\n",
    "\n",
    "    sentence = sentence.lower()\n",
    "    sentence = ' '.join(\n",
    "        re.sub('(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)',' ',\n",
    "               sentence).split())\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    words = tokenizer.tokenize(sentence) \n",
    "    good_words = [w for w in words]\n",
    "    return ' '.join(good_words)\n",
    "\n",
    "\n",
    "def complete_preprocessing(sentence, language):\n",
    "    return quick_preprocessing(sentence, language, True, True, True, True)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "QgvwnaEy2WsO"
   },
   "source": [
    "# Apply preprocessing to train and test samples.\n",
    "preprocessed_training = []\n",
    "preprocessed_testing = []\n",
    "\n",
    "pbar = tqdm(total=len(train_examples), position=0, leave=True)\n",
    "for tr_example, ts_example in zip(train_examples, test_examples):\n",
    "    preprocessed_training.append(complete_preprocessing(tr_example, 'english'))\n",
    "    preprocessed_testing.append(complete_preprocessing(ts_example, 'english'))\n",
    "    pbar.update(1)\n",
    "\n",
    "pbar.close()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kpu0HOA1THvU"
   },
   "source": [
    "We use the same code as before for performing the training"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "iy2FB1Y738B3"
   },
   "source": [
    "train_loop(preprocessed_training, preprocessed_testing, train_labels, \n",
    "           test_labels)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DjoOk0LK11Hk"
   },
   "source": [
    "## Entering BERT Embeddings [EXERCISE]\n",
    "\n",
    "Recently, pretrained sentence embeddings, lead by Bidirectional Encoder Representations from Transformers ([BERT](https://www.aclweb.org/anthology/N19-1423/)), have become the standard in any NLP application. This means that sentences can be transformed into a dense and meaningful representation exploiting these large pretrained language models.\n",
    "\n",
    "In this section, we use  the [Sentence-Transformers](https://github.com/UKPLab/sentence-transformers) library that permits to compute dense vector representations for sentences and paragraphs using transformer networks like BERT / RoBERTa / XLM-RoBERTa etc. \n",
    "\n",
    "**NOTE**: this might generate an OOM errror because these embeddings are generated with pytroch (that is fighting with Jax for the GPU). One thing you can do is: restart the notebook, generate the embeddings, and then train this new network with jax. You can look at jax memory allocation settings [here](https://jax.readthedocs.io/en/latest/gpu_memory_allocation.html)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "orsRC8nqUpp-"
   },
   "source": [
    "%%capture\n",
    "model_bert = SentenceTransformer('distilbert-base-nli-mean-tokens')  "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "EWSA7EYCnnjb"
   },
   "source": [
    "# Transform the training and testing data into sentence embeddings.\n",
    "# This might take a few minutes.\n",
    "embeddings_train = jnp.array(model_bert.encode(train_examples, show_progress_bar=True))\n",
    "embeddings_test = jnp.array(model_bert.encode(test_examples, show_progress_bar=True))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "khtTExSl-cRg"
   },
   "source": [
    "def bert_train_loop(X_train, X_test, train_labels, test_labels, \n",
    "               layer_sizes=None):\n",
    "    # Visualize learning progress with a bar.\n",
    "    pbar = tqdm(total=NUM_EPOCHS, position=0, leave=True)\n",
    "\n",
    "    if layer_sizes is None:\n",
    "        layer_sizes = [X_train.shape[1], 256,  2]\n",
    "    params = init_network_params(layer_sizes, random.PRNGKey(0))\n",
    "\n",
    "    # Transform target data to one-hot representation.\n",
    "    y = jax.nn.one_hot(train_labels, N_TARGETS)\n",
    "    y_test = jax.nn.one_hot(test_labels, N_TARGETS)\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        # Update parameters for each batch.\n",
    "        for x_t, y_t in batch(X_train, y):\n",
    "            params, loss_value = update(params, x_t, y_t)\n",
    "        pbar.update(1)\n",
    "        \n",
    "        # Compute accuracy on training and test.\n",
    "        train_acc = accuracy(params, X_train, y)\n",
    "        test_acc = accuracy(params, X_test, y_test)\n",
    "\n",
    "        pbar.set_description('Loss value is {0:.2f}, training accuracy is '\n",
    "                            '{1:.5}, test accuracy is {2:.5}'.format(loss_value,\n",
    "                                                                    train_acc,\n",
    "                                                                    test_acc))\n",
    "    return params"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "3qn3SknV-kuF"
   },
   "source": [
    "bert_network = bert_train_loop(embeddings_train, embeddings_test, train_labels, test_labels)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c5wEuMBNCHVm"
   },
   "source": [
    "## Let's play with our new network :)\n",
    "\n",
    "We are going to test our sentiment model on two positive documents and 1 negative one. Let's see if the model is able to predict the classes as expected.\n",
    "You can add custom sentences and see the results!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Y4J3_tLnBaMG"
   },
   "source": [
    "examples = jnp.array(model_bert.encode([\"wow, very nice movie totally recommend it\", \n",
    "                                        \"worst movie ever, I will never go to the cinema again\", \n",
    "                                        \"I liked it a lot, best movie ever\"]))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "K542K0V6-yZE"
   },
   "source": [
    "jnp.argmax(jnp.exp(batched_predict(bert_network, examples)), axis=1)"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}